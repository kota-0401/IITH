{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":73111,"databundleVersionId":8040143,"sourceType":"competition"}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n #   for filename in filenames:\n  #      print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# Define path to your dataset\ntrain_dir = '/kaggle/input/iith-dl-contest-2024/train/train'\ntest_dir = '/kaggle/input/iith-dl-contest-2024/test/test'\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-27T16:41:06.512820Z","iopub.execute_input":"2024-04-27T16:41:06.513700Z","iopub.status.idle":"2024-04-27T16:41:06.518931Z","shell.execute_reply.started":"2024-04-27T16:41:06.513665Z","shell.execute_reply":"2024-04-27T16:41:06.517924Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, random_split\n\n# Define mean and standard deviation for normalization\nmean = [0.495, 0.455, 0.475]  # Assuming RGB images\nstd = [0.265, 0.255, 0.26]  \n\n# Define transformations\ntrain_transforms = transforms.Compose([ \n    transforms.RandomResizedCrop(64),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n    transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\nval_test_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\n# Define datasets\ntrain_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n\nclass_names = train_dataset.classes\nnum_classes = len(class_names) \n\nclass CustomTestDataset(datasets.ImageFolder):\n    def __init__(self, root, transform=None):\n        self.root = root\n        self.transform = transform\n        self.samples = os.listdir(root)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root, self.samples[idx])\n        img = Image.open(img_name)\n        if img.mode != 'RGB':  # Convert to RGB if not already\n            img = img.convert('RGB')\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, self.samples[idx]\n\ntest_dataset = CustomTestDataset(test_dir, transform=val_test_transforms)\n\n# Split train_dataset into train and validation datasets\ntrain_size = int(0.8 * len(train_dataset))\nvalidation_size = len(train_dataset) - train_size\ntrain_dataset, validation_dataset = random_split(train_dataset, [train_size, validation_size])\n\nvalidation_dataset.dataset.transform = val_test_transforms\ntrain_dataset.dataset.transform = train_transforms\n\n# Define dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\nvalidation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:43:59.492924Z","iopub.execute_input":"2024-04-27T16:43:59.493752Z","iopub.status.idle":"2024-04-27T16:44:10.585048Z","shell.execute_reply.started":"2024-04-27T16:43:59.493722Z","shell.execute_reply":"2024-04-27T16:44:10.584075Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\n\n# Define your device for training\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define your model architecture\nclass MyModel(nn.Module):\n    def __init__(self, num_classes):\n        super(MyModel, self).__init__()\n        # Load a pre-trained ResNet-152 model\n        self.base_model = models.resnet152(pretrained=True)\n        # Modify the last fully connected layer to match the number of classes in your dataset\n        num_features = self.base_model.fc.in_features\n        self.base_model.fc = nn.Linear(num_features, num_classes)\n\n    def forward(self, x):\n        return self.base_model(x)\n\n# Define your model\nmodel = MyModel(num_classes=num_classes)\nmodel.to(device)\n\n# Define your loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Define your optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Define your learning rate scheduler\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:44:15.133745Z","iopub.execute_input":"2024-04-27T16:44:15.134097Z","iopub.status.idle":"2024-04-27T16:44:18.342605Z","shell.execute_reply.started":"2024-04-27T16:44:15.134069Z","shell.execute_reply":"2024-04-27T16:44:18.341826Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n100%|██████████| 230M/230M [00:01<00:00, 166MB/s]  \n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the number of epochs\nnum_epochs = 20\n\n# Training loop\nfor epoch in range(num_epochs):\n    # Set model to training mode\n    model.train()\n    \n    # Initialize running loss\n    running_loss = 0.0\n    \n    # Iterate over the training dataset\n    for inputs, labels in train_loader:\n        # Move inputs and labels to the appropriate device\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(inputs)\n        \n        # Calculate the loss\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        \n        # Update running loss\n        running_loss += loss.item() * inputs.size(0)\n    \n    # Calculate average training loss for the epoch\n    train_loss = running_loss / len(train_loader.dataset)\n    \n    # Print training loss for the epoch\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}\")\n    \n    # Set model to evaluation mode\n    model.eval()\n    \n    # Initialize validation loss\n    val_loss = 0.0\n    \n    # Initialize correct predictions and total examples\n    correct_preds = 0\n    total_examples = 0\n    \n    # Disable gradient calculation during validation\n    with torch.no_grad():\n        # Iterate over the validation dataset\n        for inputs, labels in validation_loader:\n            # Move inputs and labels to the appropriate device\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, labels)\n            \n            # Update validation loss\n            val_loss += loss.item() * inputs.size(0)\n            \n            # Get predicted labels\n            _, predicted = torch.max(outputs, 1)\n            \n            # Update total examples\n            total_examples += labels.size(0)\n            \n            # Update correct predictions\n            correct_preds += (predicted == labels).sum().item()\n    \n    # Calculate average validation loss\n    val_loss = val_loss / len(validation_loader.dataset)\n    \n    # Calculate validation accuracy\n    val_accuracy = correct_preds / total_examples\n    \n    # Print validation loss and accuracy\n    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n    \n    # Adjust learning rate based on validation loss\n    scheduler.step(val_loss)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:45:17.242111Z","iopub.execute_input":"2024-04-27T16:45:17.242531Z","iopub.status.idle":"2024-04-27T17:51:48.834951Z","shell.execute_reply.started":"2024-04-27T16:45:17.242501Z","shell.execute_reply":"2024-04-27T17:51:48.833793Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/20, Train Loss: 3.0357\nValidation Loss: 5.9551, Validation Accuracy: 0.2743\nEpoch 2/20, Train Loss: 2.5979\nValidation Loss: 2.3927, Validation Accuracy: 0.3596\nEpoch 3/20, Train Loss: 2.3626\nValidation Loss: 2.4566, Validation Accuracy: 0.3713\nEpoch 4/20, Train Loss: 2.2235\nValidation Loss: 2.2059, Validation Accuracy: 0.4103\nEpoch 5/20, Train Loss: 2.1906\nValidation Loss: 2.3653, Validation Accuracy: 0.4080\nEpoch 6/20, Train Loss: 2.1306\nValidation Loss: 2.1225, Validation Accuracy: 0.4372\nEpoch 7/20, Train Loss: 2.0679\nValidation Loss: 2.1315, Validation Accuracy: 0.4633\nEpoch 8/20, Train Loss: 1.9570\nValidation Loss: 2.1286, Validation Accuracy: 0.4575\nEpoch 9/20, Train Loss: 1.9413\nValidation Loss: 1.9905, Validation Accuracy: 0.4832\nEpoch 10/20, Train Loss: 1.8081\nValidation Loss: 1.8769, Validation Accuracy: 0.5029\nEpoch 11/20, Train Loss: 1.8002\nValidation Loss: 2.2163, Validation Accuracy: 0.4944\nEpoch 12/20, Train Loss: 1.7355\nValidation Loss: 1.8385, Validation Accuracy: 0.5255\nEpoch 13/20, Train Loss: 1.7389\nValidation Loss: 1.8266, Validation Accuracy: 0.5151\nEpoch 14/20, Train Loss: 1.6924\nValidation Loss: 1.8096, Validation Accuracy: 0.5126\nEpoch 15/20, Train Loss: 1.6520\nValidation Loss: 1.7368, Validation Accuracy: 0.5311\nEpoch 16/20, Train Loss: 1.6163\nValidation Loss: 1.6828, Validation Accuracy: 0.5435\nEpoch 17/20, Train Loss: 1.6253\nValidation Loss: 1.8959, Validation Accuracy: 0.5419\nEpoch 18/20, Train Loss: 1.5933\nValidation Loss: 1.6922, Validation Accuracy: 0.5494\nEpoch 19/20, Train Loss: 1.5450\nValidation Loss: 1.7316, Validation Accuracy: 0.5541\nEpoch 20/20, Train Loss: 1.5002\nValidation Loss: 1.6971, Validation Accuracy: 0.5537\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the number of epochs\nnum_epochs = 5\n\n# Training loop\nfor epoch in range(num_epochs):\n    # Set model to training mode\n    model.train()\n    \n    # Initialize running loss\n    running_loss = 0.0\n    \n    # Iterate over the training dataset\n    for inputs, labels in train_loader:\n        # Move inputs and labels to the appropriate device\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(inputs)\n        \n        # Calculate the loss\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        \n        # Update running loss\n        running_loss += loss.item() * inputs.size(0)\n    \n    # Calculate average training loss for the epoch\n    train_loss = running_loss / len(train_loader.dataset)\n    \n    # Print training loss for the epoch\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}\")\n    \n    # Set model to evaluation mode\n    model.eval()\n    \n    # Initialize validation loss\n    val_loss = 0.0\n    \n    # Initialize correct predictions and total examples\n    correct_preds = 0\n    total_examples = 0\n    \n    # Disable gradient calculation during validation\n    with torch.no_grad():\n        # Iterate over the validation dataset\n        for inputs, labels in validation_loader:\n            # Move inputs and labels to the appropriate device\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, labels)\n            \n            # Update validation loss\n            val_loss += loss.item() * inputs.size(0)\n            \n            # Get predicted labels\n            _, predicted = torch.max(outputs, 1)\n            \n            # Update total examples\n            total_examples += labels.size(0)\n            \n            # Update correct predictions\n            correct_preds += (predicted == labels).sum().item()\n    \n    # Calculate average validation loss\n    val_loss = val_loss / len(validation_loader.dataset)\n    \n    # Calculate validation accuracy\n    val_accuracy = correct_preds / total_examples\n    \n    # Print validation loss and accuracy\n    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n    \n    # Adjust learning rate based on validation loss\n    scheduler.step(val_loss)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T17:57:32.791694Z","iopub.execute_input":"2024-04-27T17:57:32.792071Z","iopub.status.idle":"2024-04-27T18:14:05.417159Z","shell.execute_reply.started":"2024-04-27T17:57:32.792041Z","shell.execute_reply":"2024-04-27T18:14:05.415943Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch 1/5, Train Loss: 1.5372\nValidation Loss: 2.2192, Validation Accuracy: 0.4797\nEpoch 2/5, Train Loss: 1.5411\nValidation Loss: 1.5647, Validation Accuracy: 0.5828\nEpoch 3/5, Train Loss: 1.4411\nValidation Loss: 1.7368, Validation Accuracy: 0.5369\nEpoch 4/5, Train Loss: 1.5146\nValidation Loss: 2.0808, Validation Accuracy: 0.5346\nEpoch 5/5, Train Loss: 1.4274\nValidation Loss: 1.6345, Validation Accuracy: 0.5848\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the number of epochs\nnum_epochs = 5\n\n# Training loop\nfor epoch in range(num_epochs):\n    # Set model to training mode\n    model.train()\n    \n    # Initialize running loss\n    running_loss = 0.0\n    \n    # Iterate over the training dataset\n    for inputs, labels in train_loader:\n        # Move inputs and labels to the appropriate device\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(inputs)\n        \n        # Calculate the loss\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        \n        # Update running loss\n        running_loss += loss.item() * inputs.size(0)\n    \n    # Calculate average training loss for the epoch\n    train_loss = running_loss / len(train_loader.dataset)\n    \n    # Print training loss for the epoch\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}\")\n    \n    # Set model to evaluation mode\n    model.eval()\n    \n    # Initialize validation loss\n    val_loss = 0.0\n    \n    # Initialize correct predictions and total examples\n    correct_preds = 0\n    total_examples = 0\n    \n    # Disable gradient calculation during validation\n    with torch.no_grad():\n        # Iterate over the validation dataset\n        for inputs, labels in validation_loader:\n            # Move inputs and labels to the appropriate device\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, labels)\n            \n            # Update validation loss\n            val_loss += loss.item() * inputs.size(0)\n            \n            # Get predicted labels\n            _, predicted = torch.max(outputs, 1)\n            \n            # Update total examples\n            total_examples += labels.size(0)\n            \n            # Update correct predictions\n            correct_preds += (predicted == labels).sum().item()\n    \n    # Calculate average validation loss\n    val_loss = val_loss / len(validation_loader.dataset)\n    \n    # Calculate validation accuracy\n    val_accuracy = correct_preds / total_examples\n    \n    # Print validation loss and accuracy\n    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n    \n    # Adjust learning rate based on validation loss\n    scheduler.step(val_loss)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T18:19:27.580647Z","iopub.execute_input":"2024-04-27T18:19:27.581645Z","iopub.status.idle":"2024-04-27T18:35:49.636952Z","shell.execute_reply.started":"2024-04-27T18:19:27.581606Z","shell.execute_reply":"2024-04-27T18:35:49.635682Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch 1/5, Train Loss: 1.4054\nValidation Loss: 5.1165, Validation Accuracy: 0.5050\nEpoch 2/5, Train Loss: 1.5748\nValidation Loss: 1.6161, Validation Accuracy: 0.5742\nEpoch 3/5, Train Loss: 1.3783\nValidation Loss: 1.6984, Validation Accuracy: 0.5917\nEpoch 4/5, Train Loss: 1.2445\nValidation Loss: 1.5253, Validation Accuracy: 0.6192\nEpoch 5/5, Train Loss: 1.1967\nValidation Loss: 1.7541, Validation Accuracy: 0.6207\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize lists to store filenames and predicted categories\ntest_filenames = []\ntest_predictions = []\n\n# Set model to evaluation mode\nmodel.eval()\n\n# Disable gradient calculation during testing\nwith torch.no_grad():\n    # Iterate over the test dataset\n    for inputs, filenames in test_loader:\n        \n        # Move inputs to the appropriate device\n        inputs = inputs.to(device)\n        \n        # Forward pass\n        outputs = model(inputs)\n        \n        # Get predicted labels\n        _, predicted = torch.max(outputs, 1)\n        \n        # Convert tensor to numpy array and then to list\n        filenames = [filename for filename in filenames]\n        \n        # Extend lists with filenames and predictions along with class names\n        for filename, prediction in zip(filenames, predicted.cpu().numpy()):\n            test_filenames.append(filename)\n            test_predictions.append(class_names[prediction])\n        \n# Create a DataFrame with filenames and predicted categories\ntest_df = pd.DataFrame({'ID': test_filenames, 'Category': test_predictions})\n\n# Save the DataFrame to a CSV file\ntest_df.to_csv('/kaggle/working/predictions.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T18:35:49.639428Z","iopub.execute_input":"2024-04-27T18:35:49.639844Z","iopub.status.idle":"2024-04-27T18:36:27.408236Z","shell.execute_reply.started":"2024-04-27T18:35:49.639805Z","shell.execute_reply":"2024-04-27T18:36:27.407095Z"},"trusted":true},"execution_count":18,"outputs":[]}]}